---
title: "Qno_8"
author: "Arpan Sapkota"
date: "2023-07-02"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Error due to some issue, couldnt able to Knit into HTML so Please check this Markdown File
## a
```{r, warning=FALSE}
data("Arrests")
set.seed(07)

train_indices <- sample(1:nrow(Arrests), 0.8 * nrow(Arrests))

train_data <- Arrests[train_indices, ]
test_data <- Arrests[-train_indices, ]


```

## b
```{r, warning=FALSE}

library(naivebayes)
library(caret)

logreg_model <- glm(released ~ colour + age + sex + employed + citizen, data = train_data, family = binomial)

nb_model <- naive_bayes(released ~ colour + age + sex + employed + citizen, data = train_data)

```

## c
```{r, warning=FALSE}
logreg_pred <- predict(logreg_model, newdata = train_data, type = "response")
logreg_pred <- ifelse(logreg_pred > 0.5, "Yes", "No")

#logreg_cm <- confusionMatrix(logreg_pred, train_data$released)
#logreg_sensitivity <- logreg_cm$byClass[["Sensitivity"]]
#logreg_specificity <- logreg_cm$byClass[["Specificity"]]

#nb_pred <- predict(nb_model, newdata = train_data, type = "raw")
#nb_pred <- ifelse(nb_pred$Yes > nb_pred$No, "Yes", "No")

#nb_cm <- confusionMatrix(nb_pred, train_data$released)
#nb_sensitivity <- nb_cm$byClass[["Sensitivity"]]
#nb_specificity <- nb_cm$byClass[["Specificity"]]

```
The confusion matrix provides information about the performance of the models. It shows the number of true positive, true negative, false positive, and false negative predictions.
Sensitivity (also called recall or true positive rate) measures the proportion of actual positive cases that are correctly predicted as positive by the model. Higher sensitivity indicates a lower rate of false negatives.
Specificity (also called true negative rate) measures the proportion of actual negative cases that are correctly predicted as negative by the model. Higher specificity indicates a lower rate of false positives.


## d
```{r, warning=FALSE}
test_logreg_pred <- predict(logreg_model, newdata = test_data, type = "response")
test_logreg_pred <- ifelse(test_logreg_pred > 0.5, "Yes", "No")

#test_nb_pred <- predict(nb_model, newdata = test_data, type = "raw")
#test_nb_pred <- ifelse(test_nb_pred$Yes > test_nb_pred$No, "Yes", "No")

```

## e
```{r, warning=FALSE}
#test_logreg_cm <- confusionMatrix(test_logreg_pred, test_data

```

